"""
Shield-Ryzen V2 -- Threshold Optimization
==========================================
Grid search on validation set to find optimal confidence threshold.
Proves the original 89% threshold was arbitrary by comparing against
data-driven optimal thresholds at different operating points.

Outputs:
  - F1-optimal threshold
  - Threshold at target FAR = 0.1% (high-security)
  - Threshold at target FAR = 1.0% (balanced)
  - Comparison against old 89% threshold
  - Saves to config/decision_thresholds.yaml

Developer: Inayat Hussain | AMD Slingshot 2026
Part 3 of 14 â€” Liveness, Forensics & Decision Logic Calibration
"""

from __future__ import annotations

import json
import logging
import os
import sys
import time
from pathlib import Path
from typing import Optional

import cv2
import numpy as np
import yaml

# Add project root to path
_SCRIPT_DIR = Path(__file__).resolve().parent
_PROJECT_ROOT = _SCRIPT_DIR.parent
if str(_PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(_PROJECT_ROOT))

from shield_utils_core import setup_logger, ConfidenceCalibrator

_log = setup_logger("ThresholdOptimizer")


# ===================================================================
# Metrics
# ===================================================================

def _compute_f1(
    y_true: np.ndarray,
    y_pred: np.ndarray,
) -> tuple[float, float, float]:
    """Compute precision, recall, and F1 score.

    Positive class = REAL (label 1).
    """
    tp = int(((y_pred == 1) & (y_true == 1)).sum())
    fp = int(((y_pred == 1) & (y_true == 0)).sum())
    fn = int(((y_pred == 0) & (y_true == 1)).sum())

    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    f1 = (2 * precision * recall / (precision + recall)
          if (precision + recall) > 0 else 0.0)
    return precision, recall, f1


def _compute_far_frr(
    y_true: np.ndarray,
    scores: np.ndarray,
    threshold: float,
) -> tuple[float, float]:
    """Compute False Accept Rate and False Reject Rate.

    FAR = falsely accepting a FAKE as real
    FRR = falsely rejecting a REAL as fake
    """
    n_fake = int((y_true == 0).sum())
    n_real = int((y_true == 1).sum())

    # Predictions: score >= threshold -> REAL
    preds = (scores >= threshold).astype(int)

    false_accepts = int(((preds == 1) & (y_true == 0)).sum())
    false_rejects = int(((preds == 0) & (y_true == 1)).sum())

    far = false_accepts / n_fake if n_fake > 0 else 0.0
    frr = false_rejects / n_real if n_real > 0 else 0.0
    return far, frr


# ===================================================================
# Data Loading
# ===================================================================

def _load_validation_data(
    data_path: str,
) -> tuple[np.ndarray, np.ndarray]:
    """Load validation scores and labels from a directory or JSON file.

    Supports:
      - Directory with real/ and fake/ subfolders
      - JSON file with {scores: [...], labels: [...]}
      - Synthetic generation if no data found
    """
    if os.path.isfile(data_path) and data_path.endswith('.json'):
        with open(data_path, 'r') as f:
            data = json.load(f)
        return np.array(data['labels']), np.array(data['scores'])

    if os.path.isdir(data_path):
        _log.info("Loading validation data from directory: %s", data_path)
        # Look for pre-computed scores
        scores_file = os.path.join(data_path, 'scores.json')
        if os.path.exists(scores_file):
            with open(scores_file, 'r') as f:
                data = json.load(f)
            return np.array(data['labels']), np.array(data['scores'])

    # Generate synthetic validation data for threshold optimization
    # WHY: Demonstrates the mechanism even without a real validation set.
    # The thresholds WILL be re-optimized in Part 10 with real data.
    _log.warning(
        "No validation data found at %s. Using synthetic data "
        "to demonstrate threshold optimization mechanism.", data_path
    )
    return _generate_synthetic_scores()


def _generate_synthetic_scores(
    n_real: int = 500,
    n_fake: int = 500,
    seed: int = 42,
) -> tuple[np.ndarray, np.ndarray]:
    """Generate synthetic model output scores for testing.

    Simulates a reasonably well-performing deepfake detector:
    - Real faces: scores centered around 0.85 (high real probability)
    - Fake faces: scores centered around 0.25 (low real probability)
    - Some overlap to make threshold selection non-trivial
    """
    rng = np.random.RandomState(seed)

    real_scores = np.clip(rng.normal(0.85, 0.10, n_real), 0, 1)
    fake_scores = np.clip(rng.normal(0.25, 0.12, n_fake), 0, 1)

    labels = np.concatenate([np.ones(n_real), np.zeros(n_fake)])
    scores = np.concatenate([real_scores, fake_scores])

    return labels, scores


# ===================================================================
# Threshold Optimization
# ===================================================================

def optimize_thresholds(
    model_path: Optional[str] = None,
    validation_data_path: Optional[str] = None,
    output_path: str = "config/decision_thresholds.yaml",
    old_threshold: float = 0.89,
) -> dict:
    """Find optimal confidence thresholds via grid search.

    Searches over [0.01, 0.99] in steps of 0.01 to find:
    - F1-optimal threshold (maximizes F1 score)
    - Threshold at FAR = 0.1% (high-security mode)
    - Threshold at FAR = 1.0% (balanced mode)

    Compares against old arbitrary 89% threshold to prove
    it was suboptimal.

    Args:
        model_path: Path to ONNX model (for inference mode).
        validation_data_path: Path to validation data.
        output_path: Where to save optimal thresholds YAML.
        old_threshold: The previous hardcoded threshold to compare against.

    Returns:
        Dictionary with optimal thresholds and comparison metrics.
    """
    _log.info("=" * 55)
    _log.info("SHIELD-RYZEN V2 -- THRESHOLD OPTIMIZATION")
    _log.info("=" * 55)

    # Load or generate validation data
    data_path = validation_data_path or os.path.join(
        str(_PROJECT_ROOT), "data", "validation"
    )
    labels, scores = _load_validation_data(data_path)
    _log.info("  Loaded %d samples (%d real, %d fake)",
              len(labels), int((labels == 1).sum()), int((labels == 0).sum()))

    # --- Grid search ---
    thresholds = np.arange(0.01, 1.00, 0.01)
    best_f1 = 0.0
    best_threshold_f1 = 0.50
    best_precision = 0.0
    best_recall = 0.0

    results_grid: list[dict] = []

    for thresh in thresholds:
        preds = (scores >= thresh).astype(int)
        prec, rec, f1 = _compute_f1(labels, preds)
        far, frr = _compute_far_frr(labels, scores, thresh)

        results_grid.append({
            "threshold": round(float(thresh), 2),
            "f1": round(f1, 4),
            "precision": round(prec, 4),
            "recall": round(rec, 4),
            "far": round(far, 4),
            "frr": round(frr, 4),
        })

        if f1 > best_f1:
            best_f1 = f1
            best_threshold_f1 = float(thresh)
            best_precision = prec
            best_recall = rec

    # --- Find threshold at target FAR levels ---
    threshold_far_01 = _find_threshold_at_far(labels, scores, target_far=0.001)
    threshold_far_10 = _find_threshold_at_far(labels, scores, target_far=0.01)

    # --- Compare against old 89% threshold ---
    old_preds = (scores >= old_threshold).astype(int)
    old_prec, old_rec, old_f1 = _compute_f1(labels, old_preds)
    old_far, old_frr = _compute_far_frr(labels, scores, old_threshold)

    # --- Build results ---
    results = {
        "optimization_timestamp": time.strftime("%Y-%m-%dT%H:%M:%S"),
        "data_samples": int(len(labels)),
        "data_real": int((labels == 1).sum()),
        "data_fake": int((labels == 0).sum()),

        "confidence_threshold": round(best_threshold_f1, 2),
        "f1_optimal": {
            "threshold": round(best_threshold_f1, 2),
            "f1": round(best_f1, 4),
            "precision": round(best_precision, 4),
            "recall": round(best_recall, 4),
        },
        "far_0_1_pct": {
            "threshold": round(threshold_far_01, 2),
            "description": "Threshold at FAR = 0.1% (high-security)",
        },
        "far_1_0_pct": {
            "threshold": round(threshold_far_10, 2),
            "description": "Threshold at FAR = 1.0% (balanced)",
        },
        "old_89_pct": {
            "threshold": old_threshold,
            "f1": round(old_f1, 4),
            "precision": round(old_prec, 4),
            "recall": round(old_rec, 4),
            "far": round(old_far, 4),
            "frr": round(old_frr, 4),
        },

        "improvement": {
            "f1_delta": round(best_f1 - old_f1, 4),
            "threshold_differs": abs(best_threshold_f1 - old_threshold) > 0.01,
            "note": (
                f"Optimal threshold {best_threshold_f1:.2f} differs from "
                f"old {old_threshold:.2f} by {abs(best_threshold_f1 - old_threshold):.2f}"
            ),
        },

        # Configurable thresholds for the decision engine
        "laplacian_threshold": 50,     # Updated by calibration
        "blink_threshold": 0.21,       # From config
        "hysteresis_frames": 5,        # State machine stability
    }

    # Save to YAML
    full_output_path = os.path.join(str(_PROJECT_ROOT), output_path)
    os.makedirs(os.path.dirname(full_output_path), exist_ok=True)
    with open(full_output_path, 'w', encoding='utf-8') as f:
        yaml.dump(results, f, default_flow_style=False, sort_keys=False)

    _log.info("  F1-optimal threshold: %.2f (F1=%.4f)", best_threshold_f1, best_f1)
    _log.info("  Old 89%% threshold:   F1=%.4f", old_f1)
    _log.info("  Improvement:          F1 +%.4f", best_f1 - old_f1)
    _log.info("  Saved to: %s", full_output_path)
    _log.info("=" * 55)

    return results


def _find_threshold_at_far(
    labels: np.ndarray,
    scores: np.ndarray,
    target_far: float,
) -> float:
    """Find the highest threshold that achieves target FAR or below."""
    best_threshold = 0.5
    for thresh in np.arange(0.01, 1.00, 0.005):
        far, _ = _compute_far_frr(labels, scores, thresh)
        if far <= target_far:
            best_threshold = float(thresh)
            break
    return best_threshold


# ===================================================================
# Main Entry Point
# ===================================================================

def main() -> None:
    """Run threshold optimization and save results."""
    results = optimize_thresholds()

    # Also save temperature scaling params
    calibrator = ConfidenceCalibrator(temperature=1.5)
    params_path = os.path.join(str(_PROJECT_ROOT), 'config', 'temperature_params.json')
    calibrator.save_params(params_path)
    _log.info("Temperature scaling params saved (T=%.2f)", calibrator.temperature)

    print("\n--- RESULTS SUMMARY ---")
    print(f"  Optimal threshold:  {results['confidence_threshold']:.2f}")
    print(f"  F1 score:           {results['f1_optimal']['f1']:.4f}")
    print(f"  Old 89% F1:         {results['old_89_pct']['f1']:.4f}")
    print(f"  Improvement:        {results['improvement']['f1_delta']:+.4f}")
    print(f"  Threshold differs:  {results['improvement']['threshold_differs']}")
    print(f"  Config saved to:    config/decision_thresholds.yaml")


if __name__ == "__main__":
    main()
